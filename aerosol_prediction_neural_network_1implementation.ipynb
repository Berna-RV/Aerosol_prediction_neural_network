{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 10\n",
      "Number of instances: 10438\n",
      "Epoch 1/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 62ms/step - loss: 2.0562 - mae: 1.0101 - val_loss: 0.0380 - val_mae: 0.1111 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.2380 - mae: 0.3521 - val_loss: 0.0343 - val_mae: 0.1043 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0835 - mae: 0.1892 - val_loss: 0.0331 - val_mae: 0.0974 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0449 - mae: 0.1302 - val_loss: 0.0296 - val_mae: 0.0915 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0354 - mae: 0.1142 - val_loss: 0.0279 - val_mae: 0.0951 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0316 - mae: 0.1081 - val_loss: 0.0280 - val_mae: 0.0946 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0314 - mae: 0.1050 - val_loss: 0.0270 - val_mae: 0.0958 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0270 - mae: 0.0997 - val_loss: 0.0253 - val_mae: 0.0992 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0274 - mae: 0.1005 - val_loss: 0.0263 - val_mae: 0.0943 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0253 - mae: 0.0968 - val_loss: 0.0257 - val_mae: 0.0991 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0241 - mae: 0.0947 - val_loss: 0.0241 - val_mae: 0.1000 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0230 - mae: 0.0947 - val_loss: 0.0236 - val_mae: 0.0947 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0227 - mae: 0.0921 - val_loss: 0.0226 - val_mae: 0.0999 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0238 - mae: 0.0911 - val_loss: 0.0252 - val_mae: 0.1102 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0232 - mae: 0.0917 - val_loss: 0.0240 - val_mae: 0.0975 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0226 - mae: 0.0896 - val_loss: 0.0413 - val_mae: 0.1550 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0217 - mae: 0.0892 - val_loss: 0.0263 - val_mae: 0.1093 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0242 - mae: 0.0905 - val_loss: 0.0203 - val_mae: 0.0863 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0211 - mae: 0.0871 - val_loss: 0.0382 - val_mae: 0.1425 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0227 - mae: 0.0909 - val_loss: 0.0214 - val_mae: 0.0987 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0232 - mae: 0.0910 - val_loss: 0.0218 - val_mae: 0.0999 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0209 - mae: 0.0883 - val_loss: 0.0202 - val_mae: 0.0896 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0223 - mae: 0.0891 - val_loss: 0.0226 - val_mae: 0.0988 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0218 - mae: 0.0885 - val_loss: 0.0213 - val_mae: 0.0915 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0246 - mae: 0.0885 - val_loss: 0.0180 - val_mae: 0.0785 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0186 - mae: 0.0827 - val_loss: 0.0267 - val_mae: 0.0868 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0206 - mae: 0.0860 - val_loss: 0.0204 - val_mae: 0.0853 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0188 - mae: 0.0827 - val_loss: 0.0244 - val_mae: 0.0819 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0185 - mae: 0.0828 - val_loss: 0.0254 - val_mae: 0.0995 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0195 - mae: 0.0834 - val_loss: 0.0184 - val_mae: 0.0897 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0177 - mae: 0.0805 - val_loss: 0.0194 - val_mae: 0.0927 - learning_rate: 5.0000e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0184 - mae: 0.0820 - val_loss: 0.0210 - val_mae: 0.0805 - learning_rate: 5.0000e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0173 - mae: 0.0805 - val_loss: 0.0175 - val_mae: 0.0860 - learning_rate: 5.0000e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0165 - mae: 0.0779 - val_loss: 0.0207 - val_mae: 0.0909 - learning_rate: 5.0000e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0161 - mae: 0.0771 - val_loss: 0.0158 - val_mae: 0.0778 - learning_rate: 5.0000e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0165 - mae: 0.0783 - val_loss: 0.0260 - val_mae: 0.1117 - learning_rate: 5.0000e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0162 - mae: 0.0794 - val_loss: 0.0170 - val_mae: 0.0824 - learning_rate: 5.0000e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0153 - mae: 0.0772 - val_loss: 0.0199 - val_mae: 0.0908 - learning_rate: 5.0000e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0163 - mae: 0.0777 - val_loss: 0.0212 - val_mae: 0.0808 - learning_rate: 5.0000e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0166 - mae: 0.0795 - val_loss: 0.0279 - val_mae: 0.1168 - learning_rate: 5.0000e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0146 - mae: 0.0755 - val_loss: 0.0147 - val_mae: 0.0763 - learning_rate: 2.5000e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0172 - mae: 0.0788 - val_loss: 0.0261 - val_mae: 0.1017 - learning_rate: 2.5000e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0160 - mae: 0.0771 - val_loss: 0.0151 - val_mae: 0.0788 - learning_rate: 2.5000e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0151 - mae: 0.0749 - val_loss: 0.0180 - val_mae: 0.0782 - learning_rate: 2.5000e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0179 - mae: 0.0775 - val_loss: 0.0224 - val_mae: 0.1038 - learning_rate: 2.5000e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0169 - mae: 0.0778 - val_loss: 0.0171 - val_mae: 0.0827 - learning_rate: 2.5000e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0150 - mae: 0.0744 - val_loss: 0.0150 - val_mae: 0.0770 - learning_rate: 1.2500e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0161 - mae: 0.0756 - val_loss: 0.0155 - val_mae: 0.0796 - learning_rate: 1.2500e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0156 - mae: 0.0752 - val_loss: 0.0214 - val_mae: 0.0993 - learning_rate: 1.2500e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0136 - mae: 0.0726 - val_loss: 0.0160 - val_mae: 0.0780 - learning_rate: 1.2500e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0143 - mae: 0.0732 - val_loss: 0.0151 - val_mae: 0.0753 - learning_rate: 1.2500e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0145 - mae: 0.0774\n",
      "Validation MAE: 0.07634811103343964\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0120 - mae: 0.0724\n",
      "Test MAE: 0.0765424445271492\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tifffile as tiff\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Input, Concatenate, BatchNormalization, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"datasets/train.csv\")\n",
    "\n",
    "print(f'Number of features: {df.shape[1]}')\n",
    "print(f'Number of instances: {df.shape[0]}')\n",
    "df.head()\n",
    "\n",
    "# Preprocess numerical data\n",
    "numerical_features = df[['elevation', 'ozone', 'NO2', 'azimuth', 'zenith', 'incidence_azimuth', 'incidence_zenith']]\n",
    "\n",
    "# Numerical data scaling\n",
    "scaler = StandardScaler()\n",
    "numerical_features = scaler.fit_transform(numerical_features)\n",
    "\n",
    "# Function to load and preprocess image data\n",
    "def load_and_preprocess_image(filepath):\n",
    "    img = tiff.imread(filepath)\n",
    "    img_array = np.array(img)\n",
    "    img_array = img_array / 65535.0   # Normalize pixel values\n",
    "    return img_array\n",
    "\n",
    "# Load image data\n",
    "image_data = np.array([load_and_preprocess_image(os.path.join('./train/', filename)) for filename in df['file_name_l1']])\n",
    "\n",
    "# Target variable\n",
    "target = df['value_550'].values\n",
    "\n",
    "# Split data into training, validation, and testing sets\n",
    "X_train_num, X_temp_num, X_train_img, X_temp_img, y_train, y_temp = train_test_split(numerical_features, image_data, target, test_size=0.3, random_state=42)\n",
    "X_val_num, X_test_num, X_val_img, X_test_img, y_val, y_test = train_test_split(X_temp_num, X_temp_img, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Define the CNN and dense model\n",
    "class AOTModel:\n",
    "    def __init__(self, image_shape=(19, 19, 13), num_numerical_features=7):\n",
    "        # Image processing Neural Network\n",
    "        self.image_input = Input(shape=image_shape)\n",
    "        image_processing_network = Conv2D(32, (3, 3), activation='relu')(self.image_input)\n",
    "        image_processing_network = BatchNormalization()(image_processing_network)\n",
    "        image_processing_network = MaxPooling2D((2, 2))(image_processing_network)\n",
    "        image_processing_network = Dropout(0.25)(image_processing_network)\n",
    "\n",
    "        image_processing_network = Conv2D(64, (3, 3), activation='relu')(image_processing_network)\n",
    "        image_processing_network = BatchNormalization()(image_processing_network)\n",
    "        image_processing_network = MaxPooling2D((2, 2))(image_processing_network)\n",
    "        image_processing_network = Dropout(0.25)(image_processing_network)\n",
    "\n",
    "        image_processing_network = Flatten()(image_processing_network)\n",
    "        image_processing_network = Dense(64, activation='relu')(image_processing_network)\n",
    "        image_processing_network = Dropout(0.5)(image_processing_network)\n",
    "\n",
    "        # Numerical processing Neural Network\n",
    "        self.numerical_input = Input(shape=(num_numerical_features,))\n",
    "        numerical_processing_network = Dense(64, activation='relu')(self.numerical_input)\n",
    "        numerical_processing_network = BatchNormalization()(numerical_processing_network)\n",
    "        numerical_processing_network = Dropout(0.5)(numerical_processing_network)\n",
    "        \n",
    "        # Concatenation of both networks\n",
    "        aot_network = Concatenate()([image_processing_network, numerical_processing_network])\n",
    "        aot_network = Dense(64, activation='relu')(aot_network)\n",
    "        aot_network = Dropout(0.5)(aot_network)\n",
    "        aot_network = Dense(1)(aot_network)\n",
    "\n",
    "        self.aot_network_arquitecture = aot_network\n",
    "        del image_processing_network, numerical_processing_network, aot_network\n",
    "\n",
    "    def model(self):\n",
    "        model = Model(inputs= [self.image_input, self.numerical_input], outputs=self.aot_network_arquitecture)\n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "        return model\n",
    "\n",
    "# Instantiate the model\n",
    "model = AOTModel()\n",
    "model = model.model()\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True, rotation_range=10)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    [X_train_img, X_train_num], y_train,\n",
    "    validation_data=([X_val_img, X_val_num], y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.save('aot_model.h5')\n",
    "\n",
    "# Evaluate the model\n",
    "val_loss, val_mae = model.evaluate([X_val_img, X_val_num], y_val)\n",
    "print(f'Validation MAE: {val_mae}')\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_mae = model.evaluate([X_test_img, X_test_num], y_test)\n",
    "print(f'Test MAE: {test_mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
      "   id  value_550\n",
      "0   3   0.148174\n",
      "1  25   0.177376\n",
      "2  26   0.099280\n",
      "3  27   0.114423\n",
      "4  29   0.114808\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "model = load_model('aot_model.h5')\n",
    "\n",
    "# Load the new dataset\n",
    "new_df = pd.read_csv(\"datasets/test.csv\")\n",
    "\n",
    "# Preprocess numerical data\n",
    "new_numerical_features = new_df[['elevation', 'ozone', 'NO2', 'azimuth', 'zenith', 'incidence_azimuth', 'incidence_zenith']]\n",
    "new_numerical_features = scaler.transform(new_numerical_features)\n",
    "\n",
    "# Load and preprocess new image data\n",
    "new_image_data = np.array([load_and_preprocess_image(os.path.join('./test/', filename)) for filename in new_df['file_name_l1']])\n",
    "\n",
    "# Predict values for the new data\n",
    "predictions = model.predict([new_image_data, new_numerical_features])\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "results = pd.DataFrame({\n",
    "    'id': new_df['id'],\n",
    "    'value_550': predictions.flatten()\n",
    "})\n",
    "results.to_csv('predictions.csv', index=False)\n",
    "\n",
    "print(results.head())   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
