{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aerosol Optical Thickness Prediction Neural Network\n",
    "\n",
    "+ Work realized by Bernardo Vitorino and João Condeço."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Caracterization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 10\n",
      "Number of instances: 10438\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>elevation</th>\n",
       "      <th>ozone</th>\n",
       "      <th>NO2</th>\n",
       "      <th>azimuth</th>\n",
       "      <th>zenith</th>\n",
       "      <th>incidence_azimuth</th>\n",
       "      <th>incidence_zenith</th>\n",
       "      <th>file_name_l1</th>\n",
       "      <th>value_550</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>318</td>\n",
       "      <td>0.248</td>\n",
       "      <td>150.6</td>\n",
       "      <td>31.8</td>\n",
       "      <td>286.1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>AAOT_45-3139_12-5083_COPERNICUS_S2_20180807T10...</td>\n",
       "      <td>0.277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>302</td>\n",
       "      <td>0.279</td>\n",
       "      <td>161.6</td>\n",
       "      <td>44.2</td>\n",
       "      <td>243.6</td>\n",
       "      <td>3.9</td>\n",
       "      <td>AAOT_45-3139_12-5083_COPERNICUS_S2_20180916T10...</td>\n",
       "      <td>0.201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>373</td>\n",
       "      <td>0.303</td>\n",
       "      <td>163.5</td>\n",
       "      <td>34.4</td>\n",
       "      <td>103.9</td>\n",
       "      <td>9.8</td>\n",
       "      <td>AAOT_45-3139_12-5083_COPERNICUS_S2_20190421T10...</td>\n",
       "      <td>0.169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>342</td>\n",
       "      <td>0.271</td>\n",
       "      <td>144.7</td>\n",
       "      <td>25.3</td>\n",
       "      <td>286.2</td>\n",
       "      <td>7.9</td>\n",
       "      <td>AAOT_45-3139_12-5083_COPERNICUS_S2_20190623T10...</td>\n",
       "      <td>0.107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>327</td>\n",
       "      <td>0.252</td>\n",
       "      <td>140.4</td>\n",
       "      <td>29.4</td>\n",
       "      <td>105.8</td>\n",
       "      <td>7.0</td>\n",
       "      <td>AAOT_45-3139_12-5083_COPERNICUS_S2_20190720T10...</td>\n",
       "      <td>0.188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  elevation  ozone    NO2  azimuth  zenith  incidence_azimuth  \\\n",
       "0   1         10    318  0.248    150.6    31.8              286.1   \n",
       "1   2         10    302  0.279    161.6    44.2              243.6   \n",
       "2   4         10    373  0.303    163.5    34.4              103.9   \n",
       "3   5         10    342  0.271    144.7    25.3              286.2   \n",
       "4   6         10    327  0.252    140.4    29.4              105.8   \n",
       "\n",
       "   incidence_zenith                                       file_name_l1  \\\n",
       "0               8.0  AAOT_45-3139_12-5083_COPERNICUS_S2_20180807T10...   \n",
       "1               3.9  AAOT_45-3139_12-5083_COPERNICUS_S2_20180916T10...   \n",
       "2               9.8  AAOT_45-3139_12-5083_COPERNICUS_S2_20190421T10...   \n",
       "3               7.9  AAOT_45-3139_12-5083_COPERNICUS_S2_20190623T10...   \n",
       "4               7.0  AAOT_45-3139_12-5083_COPERNICUS_S2_20190720T10...   \n",
       "\n",
       "   value_550  \n",
       "0      0.277  \n",
       "1      0.201  \n",
       "2      0.169  \n",
       "3      0.107  \n",
       "4      0.188  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"datasets/train.csv\")\n",
    "\n",
    "print(f'Number of features: {df.shape[1]}')\n",
    "print(f'Number of instances: {df.shape[0]}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the dataset is composed of 10 features and has 10438 instances or observations.\n",
    "\n",
    "The target feature is the 'value_550', the one we want to be capable of predicting.\n",
    "\n",
    "Then we can split the features into two groups, the numeric features, and the image feature. The features, 'id' (identification feature, is not important for the training and prediction), 'elevation', 'ozone', 'NO2', 'azimuth', 'zenith', 'incidence_azimuth', and 'incident_zenith' are the numeric features, that will be scaled for a better Neural Network Model training. At last, but not least, the feature file_name_l1 is the name associated to the image from the zone where the other features were measured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data separation (numerical and images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import os\n",
    "import tifffile as tiff\n",
    "\n",
    "# Preprocess numerical data\n",
    "numerical_features = df[['elevation', 'ozone', 'NO2', 'azimuth', 'zenith', 'incidence_azimuth', 'incidence_zenith']]\n",
    "\n",
    "# Numerical data scaling\n",
    "scaler = StandardScaler()\n",
    "numerical_features = scaler.fit_transform(numerical_features)\n",
    "\n",
    "# Function to load and preprocess image data\n",
    "def load_and_preprocess_image(filepath):\n",
    "    img = tiff.imread(filepath)\n",
    "    img_array = np.array(img)\n",
    "    img_array = img_array / 65535.0   # Normalize pixel values\n",
    "    return img_array\n",
    "\n",
    "# Load image data\n",
    "image_data = np.array([load_and_preprocess_image(os.path.join('./train/', filename)) for filename in df['file_name_l1']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data preprocessing section we separate the numerical data from the images data. The numerical data was normalized, using the StandardScaler. Here we also created a function to load and normalize the images and put them in a numpy array.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Target variable\n",
    "target = df['value_550'].values\n",
    "\n",
    "# Split data into training, validation, and testing sets\n",
    "X_train_num, X_temp_num, X_train_img, X_temp_img, y_train, y_temp = train_test_split(numerical_features, image_data, target, test_size=0.3, random_state=42)\n",
    "X_val_num, X_test_num, X_val_img, X_test_img, y_val, y_test = train_test_split(X_temp_num, X_temp_img, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A split of the dataset into train, test and validation datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks Arquitectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 22:11:15.619374: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-24 22:11:16.856671: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Input, Concatenate, BatchNormalization, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Define the CNN and dense model\n",
    "class AOTModel:\n",
    "    def __init__(self, image_shape=(19, 19, 13), num_numerical_features=7):\n",
    "        # Image processing Neural Network\n",
    "        self.image_input = Input(shape=image_shape)\n",
    "        image_processing_network = Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(0.01))(self.image_input)\n",
    "        image_processing_network = BatchNormalization()(image_processing_network)\n",
    "        image_processing_network = MaxPooling2D((2, 2))(image_processing_network)\n",
    "        image_processing_network = Dropout(0.25)(image_processing_network)\n",
    "\n",
    "        image_processing_network = Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.01))(image_processing_network)\n",
    "        image_processing_network = BatchNormalization()(image_processing_network)\n",
    "        image_processing_network = MaxPooling2D((2, 2))(image_processing_network)\n",
    "        image_processing_network = Dropout(0.25)(image_processing_network)\n",
    "\n",
    "        image_processing_network = Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(0.01))(image_processing_network)\n",
    "        image_processing_network = BatchNormalization()(image_processing_network)\n",
    "        image_processing_network = GlobalAveragePooling2D()(image_processing_network)\n",
    "        image_processing_network = Dropout(0.5)(image_processing_network)\n",
    "\n",
    "        # Numerical processing Neural Network\n",
    "        self.numerical_input = Input(shape=(num_numerical_features,))\n",
    "        numerical_processing_network = Dense(128, activation='relu')(self.numerical_input)\n",
    "        numerical_processing_network = BatchNormalization()(numerical_processing_network)\n",
    "        numerical_processing_network = Dropout(0.5)(numerical_processing_network)\n",
    "        \n",
    "        numerical_processing_network = Dense(64, activation='relu')(numerical_processing_network)\n",
    "        numerical_processing_network = BatchNormalization()(numerical_processing_network)\n",
    "        numerical_processing_network = Dropout(0.5)(numerical_processing_network)\n",
    "\n",
    "        numerical_processing_network = Dense(32, activation='relu')(numerical_processing_network)\n",
    "        numerical_processing_network = BatchNormalization()(numerical_processing_network)\n",
    "        numerical_processing_network = Dropout(0.5)(numerical_processing_network)\n",
    "        \n",
    "        # Concatenation of both networks\n",
    "        aot_network = Concatenate()([image_processing_network, numerical_processing_network])\n",
    "        aot_network = Dense(64, activation='relu')(aot_network)\n",
    "        aot_network = Dropout(0.5)(aot_network)\n",
    "        aot_network = Dense(1)(aot_network)\n",
    "\n",
    "        self.aot_network_arquitecture = aot_network\n",
    "        del image_processing_network, numerical_processing_network, aot_network\n",
    "\n",
    "    def model(self):\n",
    "        model = Model(inputs= [self.image_input, self.numerical_input], outputs=self.aot_network_arquitecture)\n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam', loss='mean_absolute_error', metrics=['mae'])\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We divided the neural network architecture into two parts that later concatenate to produce just one value.\n",
    "\n",
    "This architecture is designed to handle both image data and numerical data in a single model. The image processing network consists of convolutional layers for feature extraction from images, while the numerical processing network consists of dense layers for handling numerical features. Both networks are combined and followed by additional dense layers to produce a final output, making this a versatile model suitable for tasks that require the integration of multiple data types.\n",
    "\n",
    "Here the arquitecture is described in more detail:\n",
    "\n",
    "+ Image Processing Network\n",
    "\n",
    "    + Input Layer: The model accepts images of shape (19, 19, 13).\n",
    "    + First Convolutional Block:\n",
    "        + Conv2D: 32 filters, kernel size (3, 3), ReLU activation, L2 regularization.\n",
    "        + BatchNormalization: Normalizes the outputs of the convolution.\n",
    "        + MaxPooling2D: Pool size (2, 2) to reduce spatial dimensions.\n",
    "        + Dropout: 25% to prevent overfitting.\n",
    "    + Second Convolutional Block:\n",
    "        + Conv2D: 64 filters, kernel size (3, 3), ReLU activation, L2 regularization.\n",
    "        + BatchNormalization: Normalizes the outputs of the convolution.\n",
    "        + MaxPooling2D: Pool size (2, 2) to reduce spatial dimensions.\n",
    "        + Dropout: 25% to prevent overfitting.\n",
    "    + Third Convolutional Block:\n",
    "        + Conv2D: 128 filters, kernel size (3, 3), ReLU activation, L2 regularization.\n",
    "        + BatchNormalization: Normalizes the outputs of the convolution.\n",
    "        + GlobalAveragePooling2D: Reduces each feature map to a single value.\n",
    "        + Dropout: 50% to prevent overfitting.\n",
    "\n",
    "+ Numerical Processing Network\n",
    "\n",
    "    + Input Layer: The model accepts numerical features of shape (7,).\n",
    "    + First Dense Block:\n",
    "        + Dense: 128 units, ReLU activation.\n",
    "        + BatchNormalization: Normalizes the outputs.\n",
    "        + Dropout: 50% to prevent overfitting.\n",
    "    + Second Dense Block:\n",
    "        + Dense: 64 units, ReLU activation.\n",
    "        + BatchNormalization: Normalizes the outputs.\n",
    "        + Dropout: 50% to prevent overfitting.\n",
    "    + Third Dense Block:\n",
    "        + Dense: 32 units, ReLU activation.\n",
    "        + BatchNormalization: Normalizes the outputs.\n",
    "        + Dropout: 50% to prevent overfitting.\n",
    "\n",
    "+ Combined Network\n",
    "\n",
    "    + Concatenation Layer: Concatenates the outputs of the image and numerical processing networks.\n",
    "    + Dense Block:\n",
    "        + Dense: 64 units, ReLU activation.\n",
    "        + Dropout: 50% to prevent overfitting.\n",
    "    + Output Layer:\n",
    "        + Dense: 1 unit (presumably for regression tasks).\n",
    "\n",
    "+ Model Compilation\n",
    "\n",
    "    + Optimizer: Adam.\n",
    "    + Loss Function: Mean Absolute Error (MAE).\n",
    "    + Metrics: Mean Absolute Error (MAE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquitecture Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Input, Concatenate, BatchNormalization, Dropout, GlobalAveragePooling2D, Add, LeakyReLU\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.initializers import HeNormal, HeUniform\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, Nadam\n",
    "\n",
    "# Define the CNN and dense model\n",
    "class OptimizedAOTModel:\n",
    "    def __init__(self, image_shape=(19, 19, 13), num_numerical_features=7):\n",
    "        # Image processing Neural Network\n",
    "        self.image_input = Input(shape=image_shape)\n",
    "        initializer = HeUniform()\n",
    "        \n",
    "        image_processing_network = Conv2D(32, (3, 3), kernel_regularizer=l2(0.01), kernel_initializer=initializer)(self.image_input)\n",
    "        image_processing_network = BatchNormalization()(image_processing_network)\n",
    "        image_processing_network = LeakyReLU()(image_processing_network)\n",
    "        image_processing_network = MaxPooling2D((2, 2))(image_processing_network)\n",
    "        image_processing_network = Dropout(0.25)(image_processing_network)\n",
    "\n",
    "        image_processing_network = Conv2D(64, (3, 3), kernel_regularizer=l2(0.01), kernel_initializer=initializer)(image_processing_network)\n",
    "        image_processing_network = BatchNormalization()(image_processing_network)\n",
    "        image_processing_network = LeakyReLU()(image_processing_network)\n",
    "        image_processing_network = MaxPooling2D((2, 2))(image_processing_network)\n",
    "        image_processing_network = Dropout(0.25)(image_processing_network)\n",
    "\n",
    "        image_processing_network = Conv2D(128, (3, 3), kernel_regularizer=l2(0.01), kernel_initializer=initializer)(image_processing_network)\n",
    "        image_processing_network = BatchNormalization()(image_processing_network)\n",
    "        image_processing_network = LeakyReLU()(image_processing_network)\n",
    "        image_processing_network = GlobalAveragePooling2D()(image_processing_network)\n",
    "        image_processing_network = Dropout(0.5)(image_processing_network)\n",
    "\n",
    "        # Numerical processing Neural Network\n",
    "        self.numerical_input = Input(shape=(num_numerical_features,))\n",
    "        numerical_processing_network = Dense(64, activation='relu', kernel_initializer=initializer)(self.numerical_input)\n",
    "        numerical_processing_network = BatchNormalization()(numerical_processing_network)\n",
    "        numerical_processing_network = Dropout(0.5)(numerical_processing_network)\n",
    "        \n",
    "        numerical_processing_network = Dense(128, activation='relu', kernel_initializer=initializer)(numerical_processing_network)\n",
    "        numerical_processing_network = BatchNormalization()(numerical_processing_network)\n",
    "        numerical_processing_network = Dropout(0.5)(numerical_processing_network)\n",
    "\n",
    "        numerical_processing_network = Dense(64, activation='relu', kernel_initializer=initializer)(numerical_processing_network)\n",
    "        numerical_processing_network = BatchNormalization()(numerical_processing_network)\n",
    "        numerical_processing_network = Dropout(0.5)(numerical_processing_network)\n",
    "        \n",
    "        # Concatenation of both networks\n",
    "        aot_network = Concatenate()([image_processing_network, numerical_processing_network])\n",
    "        aot_network = Dense(64, activation='relu', kernel_initializer=initializer)(aot_network)\n",
    "        aot_network = Dropout(0.5)(aot_network)\n",
    "        aot_network = Dense(1, kernel_initializer=initializer)(aot_network)\n",
    "\n",
    "        self.aot_network_arquitecture = aot_network\n",
    "        del image_processing_network, numerical_processing_network, aot_network\n",
    "\n",
    "    def model(self, learning_rate=0.001, optimizer_choice='adam'):\n",
    "        model = Model(inputs=[self.image_input, self.numerical_input], outputs=self.aot_network_arquitecture)\n",
    "        \n",
    "        if optimizer_choice == 'adam':\n",
    "            optimizer = Adam(learning_rate=learning_rate, clipnorm=1.0)\n",
    "        elif optimizer_choice == 'rmsprop':\n",
    "            optimizer = RMSprop(learning_rate=learning_rate, clipnorm=1.0)\n",
    "        elif optimizer_choice == 'nadam':\n",
    "            optimizer = Nadam(learning_rate=learning_rate, clipnorm=1.0)\n",
    "        else:\n",
    "            optimizer = Adam(learning_rate=learning_rate, clipnorm=1.0)\n",
    "        \n",
    "        model.compile(optimizer=optimizer, loss='mean_absolute_error', metrics=['mae'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization architecture is similar to the first concept. It enhances the initial multi-input model by incorporating advanced initialization techniques (HeUniform), alternative activation functions (LeakyReLU), and gradient clipping. These modifications aim to improve the training stability and performance of the model. The image processing network uses convolutional layers for feature extraction, while the numerical processing network uses dense layers. Both networks are concatenated and followed by additional dense layers to produce the final output, making this a robust model suitable for tasks requiring the integration of image and numerical data. Here is a more detailed description:\n",
    "\n",
    "+ Image Processing Network\n",
    "\n",
    "    + Input Layer: The model accepts images of shape (19, 19, 13).\n",
    "    + First Convolutional Block:\n",
    "        + Conv2D: 32 filters, kernel size (3, 3), L2 regularization, HeUniform initializer.\n",
    "        + BatchNormalization: Normalizes the outputs of the convolution.\n",
    "        + LeakyReLU: Activation function.\n",
    "        + MaxPooling2D: Pool size (2, 2) to reduce spatial dimensions.\n",
    "        + Dropout: 25% to prevent overfitting.\n",
    "    + Second Convolutional Block:\n",
    "        + Conv2D: 64 filters, kernel size (3, 3), L2 regularization, HeUniform initializer.\n",
    "        + BatchNormalization: Normalizes the outputs of the convolution.\n",
    "        + LeakyReLU: Activation function.\n",
    "        + MaxPooling2D: Pool size (2, 2) to reduce spatial dimensions.\n",
    "        + Dropout: 25% to prevent overfitting.\n",
    "    + Third Convolutional Block:\n",
    "        + Conv2D: 128 filters, kernel size (3, 3), L2 regularization, HeUniform initializer.\n",
    "        + BatchNormalization: Normalizes the outputs of the convolution.\n",
    "        + LeakyReLU: Activation function.\n",
    "        + GlobalAveragePooling2D: Reduces each feature map to a single value.\n",
    "        + Dropout: 50% to prevent overfitting.\n",
    "\n",
    "+ Numerical Processing Network\n",
    "\n",
    "    + Input Layer: The model accepts numerical features of shape (7,).\n",
    "    + First Dense Block:\n",
    "        + Dense: 64 units, ReLU activation, HeUniform initializer.\n",
    "        + BatchNormalization: Normalizes the outputs.\n",
    "        + Dropout: 50% to prevent overfitting.\n",
    "    + Second Dense Block:\n",
    "        + Dense: 128 units, ReLU activation, HeUniform initializer.\n",
    "        + BatchNormalization: Normalizes the outputs.\n",
    "        + Dropout: 50% to prevent overfitting.\n",
    "    + Third Dense Block:\n",
    "        + Dense: 64 units, ReLU activation, HeUniform initializer.\n",
    "        + BatchNormalization: Normalizes the outputs.\n",
    "        + Dropout: 50% to prevent overfitting.\n",
    "\n",
    "+ Combined Network\n",
    "\n",
    "    + Concatenation Layer: Concatenates the outputs of the image and numerical processing networks.\n",
    "    + Dense Block:\n",
    "        + Dense: 64 units, ReLU activation, HeUniform initializer.\n",
    "        + Dropout: 50% to prevent overfitting.\n",
    "    + Output Layer:\n",
    "        + Dense: 1 unit, HeUniform initializer (presumably for regression tasks).\n",
    "\n",
    "+ Model Compilation\n",
    "\n",
    "    + Optimizer: Configurable (Adam, RMSprop, Nadam) with a learning rate of 0.001 and gradient clipping (clipnorm=1.0).\n",
    "    + Loss Function: Mean Absolute Error (MAE).\n",
    "    + Metrics: Mean Absolute Error (MAE).\n",
    "\n",
    "+ Additional Techniques\n",
    "\n",
    "    + HeUniform Initializer: Used for initializing the weights, which can help in training deep networks by maintaining a better distribution of weights.\n",
    "    + LeakyReLU: Used instead of ReLU to allow a small gradient when the unit is not active, which can help in training deep networks by preventing dead neurons.\n",
    "    + Gradient Clipping: Clipping the gradients to a maximum norm of 1.0 to stabilize training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 22:11:18.296463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10689 MB memory:  -> device: 0, name: NVIDIA TITAN V, pci bus id: 0000:5e:00.0, compute capability: 7.0\n",
      "2024-06-24 22:11:18.297306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 10689 MB memory:  -> device: 1, name: NVIDIA TITAN V, pci bus id: 0000:86:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1719263483.479886 3528219 service.cc:145] XLA service 0x7fc2cc001bd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1719263483.479927 3528219 service.cc:153]   StreamExecutor device (0): NVIDIA TITAN V, Compute Capability 7.0\n",
      "I0000 00:00:1719263483.479933 3528219 service.cc:153]   StreamExecutor device (1): NVIDIA TITAN V, Compute Capability 7.0\n",
      "2024-06-24 22:11:23.608715: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-06-24 22:11:24.254298: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 22/229\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 3.2283 - mae: 1.7888"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1719263501.105922 3528219 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 92ms/step - loss: 2.6097 - mae: 1.3182 - val_loss: 1.0330 - val_mae: 0.0952 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.3100 - mae: 0.4668 - val_loss: 0.6869 - val_mae: 0.1009 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.7062 - mae: 0.1881 - val_loss: 0.4317 - val_mae: 0.0886 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.3994 - mae: 0.0991 - val_loss: 0.2822 - val_mae: 0.0875 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2561 - mae: 0.0857 - val_loss: 0.1960 - val_mae: 0.0840 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1798 - mae: 0.0808 - val_loss: 0.1592 - val_mae: 0.0896 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1430 - mae: 0.0809 - val_loss: 0.1351 - val_mae: 0.0888 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1268 - mae: 0.0846 - val_loss: 0.1156 - val_mae: 0.0846 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1090 - mae: 0.0800 - val_loss: 0.1052 - val_mae: 0.0830 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1021 - mae: 0.0810 - val_loss: 0.1103 - val_mae: 0.0930 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0972 - mae: 0.0811 - val_loss: 0.0996 - val_mae: 0.0866 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0939 - mae: 0.0813 - val_loss: 0.0900 - val_mae: 0.0801 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0901 - mae: 0.0799 - val_loss: 0.0942 - val_mae: 0.0843 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0916 - mae: 0.0826 - val_loss: 0.0959 - val_mae: 0.0879 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0874 - mae: 0.0796 - val_loss: 0.1029 - val_mae: 0.0949 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0882 - mae: 0.0804 - val_loss: 0.1244 - val_mae: 0.1173 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0855 - mae: 0.0784 - val_loss: 0.0970 - val_mae: 0.0898 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0847 - mae: 0.0782 - val_loss: 0.1156 - val_mae: 0.1106 - learning_rate: 5.0000e-04\n",
      "Epoch 19/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0811 - mae: 0.0762 - val_loss: 0.0816 - val_mae: 0.0770 - learning_rate: 5.0000e-04\n",
      "Epoch 20/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0838 - mae: 0.0792 - val_loss: 0.1030 - val_mae: 0.0984 - learning_rate: 5.0000e-04\n",
      "Epoch 21/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0789 - mae: 0.0744 - val_loss: 0.0856 - val_mae: 0.0813 - learning_rate: 5.0000e-04\n",
      "Epoch 22/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0766 - mae: 0.0722 - val_loss: 0.0782 - val_mae: 0.0736 - learning_rate: 5.0000e-04\n",
      "Epoch 23/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0796 - mae: 0.0751 - val_loss: 0.0868 - val_mae: 0.0821 - learning_rate: 5.0000e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0815 - mae: 0.0772 - val_loss: 0.0959 - val_mae: 0.0921 - learning_rate: 5.0000e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0771 - mae: 0.0732 - val_loss: 0.0924 - val_mae: 0.0880 - learning_rate: 5.0000e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0797 - mae: 0.0751 - val_loss: 0.0805 - val_mae: 0.0761 - learning_rate: 5.0000e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0770 - mae: 0.0725 - val_loss: 0.0815 - val_mae: 0.0772 - learning_rate: 5.0000e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0761 - mae: 0.0721 - val_loss: 0.0756 - val_mae: 0.0721 - learning_rate: 2.5000e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0767 - mae: 0.0733 - val_loss: 0.0945 - val_mae: 0.0911 - learning_rate: 2.5000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0740 - mae: 0.0707 - val_loss: 0.0928 - val_mae: 0.0896 - learning_rate: 2.5000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0740 - mae: 0.0708 - val_loss: 0.0731 - val_mae: 0.0700 - learning_rate: 2.5000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0749 - mae: 0.0717 - val_loss: 0.0708 - val_mae: 0.0675 - learning_rate: 2.5000e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0766 - mae: 0.0732 - val_loss: 0.0904 - val_mae: 0.0873 - learning_rate: 2.5000e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0749 - mae: 0.0718 - val_loss: 0.0726 - val_mae: 0.0693 - learning_rate: 2.5000e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0727 - mae: 0.0694 - val_loss: 0.0785 - val_mae: 0.0753 - learning_rate: 2.5000e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0728 - mae: 0.0696 - val_loss: 0.0859 - val_mae: 0.0827 - learning_rate: 2.5000e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0731 - mae: 0.0699 - val_loss: 0.1358 - val_mae: 0.1325 - learning_rate: 2.5000e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0715 - mae: 0.0683 - val_loss: 0.0793 - val_mae: 0.0763 - learning_rate: 1.2500e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0711 - mae: 0.0681 - val_loss: 0.0702 - val_mae: 0.0672 - learning_rate: 1.2500e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0725 - mae: 0.0696 - val_loss: 0.0813 - val_mae: 0.0784 - learning_rate: 1.2500e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0707 - mae: 0.0678 - val_loss: 0.0717 - val_mae: 0.0688 - learning_rate: 1.2500e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0737 - mae: 0.0709 - val_loss: 0.0859 - val_mae: 0.0830 - learning_rate: 1.2500e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0711 - mae: 0.0682 - val_loss: 0.0942 - val_mae: 0.0913 - learning_rate: 1.2500e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0701 - mae: 0.0673 - val_loss: 0.0806 - val_mae: 0.0778 - learning_rate: 1.2500e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0709 - mae: 0.0682 - val_loss: 0.0698 - val_mae: 0.0671 - learning_rate: 6.2500e-05\n",
      "Epoch 46/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0701 - mae: 0.0675 - val_loss: 0.0741 - val_mae: 0.0715 - learning_rate: 6.2500e-05\n",
      "Epoch 47/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0684 - mae: 0.0658 - val_loss: 0.0739 - val_mae: 0.0712 - learning_rate: 6.2500e-05\n",
      "Epoch 48/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0720 - mae: 0.0694 - val_loss: 0.0677 - val_mae: 0.0652 - learning_rate: 6.2500e-05\n",
      "Epoch 49/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0681 - mae: 0.0655 - val_loss: 0.0754 - val_mae: 0.0728 - learning_rate: 6.2500e-05\n",
      "Epoch 50/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0680 - mae: 0.0655 - val_loss: 0.0985 - val_mae: 0.0960 - learning_rate: 6.2500e-05\n",
      "Epoch 51/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0688 - mae: 0.0662 - val_loss: 0.0704 - val_mae: 0.0679 - learning_rate: 6.2500e-05\n",
      "Epoch 52/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0673 - mae: 0.0648 - val_loss: 0.0829 - val_mae: 0.0803 - learning_rate: 6.2500e-05\n",
      "Epoch 53/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0673 - mae: 0.0648 - val_loss: 0.0708 - val_mae: 0.0682 - learning_rate: 6.2500e-05\n",
      "Epoch 54/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0698 - mae: 0.0673 - val_loss: 0.0708 - val_mae: 0.0683 - learning_rate: 3.1250e-05\n",
      "Epoch 55/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0692 - mae: 0.0667 - val_loss: 0.0668 - val_mae: 0.0643 - learning_rate: 3.1250e-05\n",
      "Epoch 56/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0690 - mae: 0.0665 - val_loss: 0.0698 - val_mae: 0.0674 - learning_rate: 3.1250e-05\n",
      "Epoch 57/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0697 - mae: 0.0672 - val_loss: 0.0737 - val_mae: 0.0713 - learning_rate: 3.1250e-05\n",
      "Epoch 58/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0703 - mae: 0.0679 - val_loss: 0.0664 - val_mae: 0.0640 - learning_rate: 3.1250e-05\n",
      "Epoch 59/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0694 - mae: 0.0670 - val_loss: 0.0696 - val_mae: 0.0672 - learning_rate: 3.1250e-05\n",
      "Epoch 60/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0675 - mae: 0.0651 - val_loss: 0.0683 - val_mae: 0.0659 - learning_rate: 3.1250e-05\n",
      "Epoch 61/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0687 - mae: 0.0663 - val_loss: 0.0712 - val_mae: 0.0688 - learning_rate: 3.1250e-05\n",
      "Epoch 62/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0681 - mae: 0.0657 - val_loss: 0.0746 - val_mae: 0.0722 - learning_rate: 3.1250e-05\n",
      "Epoch 63/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0668 - mae: 0.0644 - val_loss: 0.0668 - val_mae: 0.0644 - learning_rate: 3.1250e-05\n",
      "Epoch 64/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0673 - mae: 0.0649 - val_loss: 0.0757 - val_mae: 0.0734 - learning_rate: 1.5625e-05\n",
      "Epoch 65/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0690 - mae: 0.0667 - val_loss: 0.0665 - val_mae: 0.0641 - learning_rate: 1.5625e-05\n",
      "Epoch 66/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0682 - mae: 0.0659 - val_loss: 0.0666 - val_mae: 0.0643 - learning_rate: 1.5625e-05\n",
      "Epoch 67/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0660 - mae: 0.0637 - val_loss: 0.0765 - val_mae: 0.0742 - learning_rate: 1.5625e-05\n",
      "Epoch 68/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0686 - mae: 0.0663 - val_loss: 0.0663 - val_mae: 0.0640 - learning_rate: 1.5625e-05\n",
      "Epoch 69/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0664 - mae: 0.0641 - val_loss: 0.0684 - val_mae: 0.0661 - learning_rate: 7.8125e-06\n",
      "Epoch 70/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0690 - mae: 0.0666 - val_loss: 0.0689 - val_mae: 0.0666 - learning_rate: 7.8125e-06\n",
      "Epoch 71/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0682 - mae: 0.0658 - val_loss: 0.0675 - val_mae: 0.0652 - learning_rate: 7.8125e-06\n",
      "Epoch 72/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0665 - mae: 0.0642 - val_loss: 0.0710 - val_mae: 0.0687 - learning_rate: 7.8125e-06\n",
      "Epoch 73/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0695 - mae: 0.0672 - val_loss: 0.0694 - val_mae: 0.0671 - learning_rate: 7.8125e-06\n",
      "Epoch 74/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0683 - mae: 0.0660 - val_loss: 0.0687 - val_mae: 0.0664 - learning_rate: 3.9063e-06\n",
      "Epoch 75/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0679 - mae: 0.0656 - val_loss: 0.0685 - val_mae: 0.0662 - learning_rate: 3.9063e-06\n",
      "Epoch 76/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0680 - mae: 0.0657 - val_loss: 0.0681 - val_mae: 0.0658 - learning_rate: 3.9063e-06\n",
      "Epoch 77/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0678 - mae: 0.0655 - val_loss: 0.0682 - val_mae: 0.0659 - learning_rate: 3.9063e-06\n",
      "Epoch 78/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0710 - mae: 0.0686 - val_loss: 0.0685 - val_mae: 0.0662 - learning_rate: 3.9063e-06\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0673 - mae: 0.0650\n",
      "Validation MAE: 0.06397651880979538\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0620 - mae: 0.0597\n",
      "Test MAE: 0.06447508931159973\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Instantiate the model\n",
    "model = AOTModel()\n",
    "model = model.model()\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    [X_train_img, X_train_num], y_train,\n",
    "    validation_data=([X_val_img, X_val_num], y_val),\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.save('aot_model.keras')\n",
    "\n",
    "# Evaluate the model\n",
    "val_loss, val_mae = model.evaluate([X_val_img, X_val_num], y_val)\n",
    "print(f'Validation MAE: {val_mae}')\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_mae = model.evaluate([X_test_img, X_test_num], y_test)\n",
    "print(f'Test MAE: {test_mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bit of code initializes and compiles an instance of the AOTModel, then trains it using both image and numerical data. It utilizes EarlyStopping and ReduceLROnPlateau callbacks to optimize training and prevent overfitting. After training, the model is saved to a file, and its performance is evaluated on both validation and test datasets. The validation and test mean absolute errors (MAE) are printed to assess the model's accuracy.\n",
    "\n",
    "+ Model Initialization and Compilation\n",
    "\n",
    "    + Model Instantiation and Compilation:\n",
    "        + model = AOTModel(): Creates an instance of the AOTModel class.\n",
    "        + model = model.model(): Calls the model method of the AOTModel instance to create a compiled Keras model.\n",
    "\n",
    "+ Callbacks\n",
    "\n",
    "    + EarlyStopping:\n",
    "        + monitor='val_loss': Monitors the validation loss.\n",
    "        + patience=10: Training stops if the validation loss doesn't improve for 10 consecutive epochs.\n",
    "        + restore_best_weights=True: Restores the model weights from the epoch with the best validation loss.\n",
    "    + ReduceLROnPlateau:\n",
    "        + monitor='val_loss': Monitors the validation loss.\n",
    "        + factor=0.5: Reduces the learning rate by a factor of 0.5 if the validation loss doesn't improve.\n",
    "        + patience=5: Waits for 5 epochs before reducing the learning rate.\n",
    "        + min_lr=1e-6: Ensures that the learning rate doesn't go below 1×10−61×10−6.\n",
    "\n",
    "+ Model Training\n",
    "\n",
    "    + Training the Model:\n",
    "        + model.fit(...): Trains the model using the provided training data.\n",
    "        + [X_train_img, X_train_num]: Input training data consisting of images and numerical features.\n",
    "        + y_train: Training labels.\n",
    "        + validation_data=([X_val_img, X_val_num], y_val): Validation data consisting of images, numerical features, and labels.\n",
    "        + epochs=200: Maximum number of epochs for training.\n",
    "        + batch_size=32: Number of samples per gradient update.\n",
    "        + callbacks=[early_stopping, reduce_lr]: List of callbacks to use during training.\n",
    "        + verbose=1: Verbosity mode for logging the progress of training.\n",
    "\n",
    "+ Model Saving\n",
    "\n",
    "    + Saving the Model:\n",
    "        + model.save('aot_model.keras'): Saves the trained model to a file named aot_model.keras.\n",
    "\n",
    "+ Model Evaluation\n",
    "\n",
    "    + Evaluating the Model on Validation Data:\n",
    "        + val_loss, val_mae = model.evaluate([X_val_img, X_val_num], y_val): Evaluates the model on the validation data and returns the validation loss and mean absolute error (MAE).\n",
    "        + print(f'Validation MAE: {val_mae}'): Prints the validation MAE.\n",
    "\n",
    "    + Evaluating the Model on Test Data:\n",
    "        + test_loss, test_mae = model.evaluate([X_test_img, X_test_num], y_test): Evaluates the model on the test data and returns the test loss and MAE.\n",
    "        + print(f'Test MAE: {test_mae}'): Prints the test MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "Epoch 1/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 100ms/step - loss: 6.2327 - mae: 1.7868 - val_loss: 4.5160 - val_mae: 0.2310 - learning_rate: 1.0000e-04\n",
      "Epoch 2/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 5.5511 - mae: 1.3238 - val_loss: 4.3152 - val_mae: 0.2712 - learning_rate: 1.0000e-04\n",
      "Epoch 3/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 5.1163 - mae: 1.1366 - val_loss: 4.0517 - val_mae: 0.2755 - learning_rate: 1.0000e-04\n",
      "Epoch 4/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 4.6602 - mae: 0.9565 - val_loss: 3.7154 - val_mae: 0.2418 - learning_rate: 1.0000e-04\n",
      "Epoch 5/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 4.2003 - mae: 0.8085 - val_loss: 3.3916 - val_mae: 0.2516 - learning_rate: 1.0000e-04\n",
      "Epoch 6/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 3.7227 - mae: 0.6697 - val_loss: 3.0173 - val_mae: 0.2278 - learning_rate: 1.0000e-04\n",
      "Epoch 7/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 3.2786 - mae: 0.5800 - val_loss: 2.6434 - val_mae: 0.2225 - learning_rate: 1.0000e-04\n",
      "Epoch 8/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 2.8326 - mae: 0.5011 - val_loss: 2.2521 - val_mae: 0.1935 - learning_rate: 1.0000e-04\n",
      "Epoch 9/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 2.3977 - mae: 0.4264 - val_loss: 1.8853 - val_mae: 0.1630 - learning_rate: 1.0000e-04\n",
      "Epoch 10/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 2.0081 - mae: 0.3607 - val_loss: 1.5728 - val_mae: 0.1369 - learning_rate: 1.0000e-04\n",
      "Epoch 11/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 1.6815 - mae: 0.3100 - val_loss: 1.3171 - val_mae: 0.1270 - learning_rate: 1.0000e-04\n",
      "Epoch 12/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 1.4006 - mae: 0.2640 - val_loss: 1.0965 - val_mae: 0.1121 - learning_rate: 1.0000e-04\n",
      "Epoch 13/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 1.1664 - mae: 0.2274 - val_loss: 0.9163 - val_mae: 0.1047 - learning_rate: 1.0000e-04\n",
      "Epoch 14/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9663 - mae: 0.1926 - val_loss: 0.7654 - val_mae: 0.0987 - learning_rate: 1.0000e-04\n",
      "Epoch 15/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.8041 - mae: 0.1691 - val_loss: 0.6418 - val_mae: 0.0962 - learning_rate: 1.0000e-04\n",
      "Epoch 16/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.6699 - mae: 0.1509 - val_loss: 0.5379 - val_mae: 0.0942 - learning_rate: 1.0000e-04\n",
      "Epoch 17/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.5539 - mae: 0.1323 - val_loss: 0.4528 - val_mae: 0.0937 - learning_rate: 1.0000e-04\n",
      "Epoch 18/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.4571 - mae: 0.1167 - val_loss: 0.3798 - val_mae: 0.0915 - learning_rate: 1.0000e-04\n",
      "Epoch 19/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.3852 - mae: 0.1122 - val_loss: 0.3194 - val_mae: 0.0892 - learning_rate: 1.0000e-04\n",
      "Epoch 20/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.3262 - mae: 0.1086 - val_loss: 0.2704 - val_mae: 0.0881 - learning_rate: 1.0000e-04\n",
      "Epoch 21/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.2685 - mae: 0.0965 - val_loss: 0.2301 - val_mae: 0.0866 - learning_rate: 1.0000e-04\n",
      "Epoch 22/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.2282 - mae: 0.0927 - val_loss: 0.1982 - val_mae: 0.0848 - learning_rate: 1.0000e-04\n",
      "Epoch 23/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1978 - mae: 0.0907 - val_loss: 0.1736 - val_mae: 0.0839 - learning_rate: 1.0000e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1713 - mae: 0.0865 - val_loss: 0.1539 - val_mae: 0.0826 - learning_rate: 1.0000e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1584 - mae: 0.0910 - val_loss: 0.1380 - val_mae: 0.0810 - learning_rate: 1.0000e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1394 - mae: 0.0852 - val_loss: 0.1257 - val_mae: 0.0796 - learning_rate: 1.0000e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1263 - mae: 0.0825 - val_loss: 0.1170 - val_mae: 0.0797 - learning_rate: 1.0000e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1186 - mae: 0.0831 - val_loss: 0.1099 - val_mae: 0.0793 - learning_rate: 1.0000e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1129 - mae: 0.0837 - val_loss: 0.1040 - val_mae: 0.0784 - learning_rate: 1.0000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1040 - mae: 0.0795 - val_loss: 0.1012 - val_mae: 0.0797 - learning_rate: 1.0000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1017 - mae: 0.0810 - val_loss: 0.0960 - val_mae: 0.0773 - learning_rate: 1.0000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0978 - mae: 0.0796 - val_loss: 0.0939 - val_mae: 0.0774 - learning_rate: 1.0000e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0928 - mae: 0.0768 - val_loss: 0.0947 - val_mae: 0.0797 - learning_rate: 1.0000e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0909 - mae: 0.0762 - val_loss: 0.1037 - val_mae: 0.0901 - learning_rate: 1.0000e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0907 - mae: 0.0775 - val_loss: 0.0886 - val_mae: 0.0763 - learning_rate: 1.0000e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0912 - mae: 0.0791 - val_loss: 0.1088 - val_mae: 0.0975 - learning_rate: 1.0000e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0883 - mae: 0.0773 - val_loss: 0.0930 - val_mae: 0.0825 - learning_rate: 1.0000e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0845 - mae: 0.0742 - val_loss: 0.0822 - val_mae: 0.0723 - learning_rate: 1.0000e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0862 - mae: 0.0766 - val_loss: 0.0859 - val_mae: 0.0769 - learning_rate: 1.0000e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0849 - mae: 0.0761 - val_loss: 0.0943 - val_mae: 0.0859 - learning_rate: 1.0000e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0790 - mae: 0.0707 - val_loss: 0.0861 - val_mae: 0.0780 - learning_rate: 1.0000e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0822 - mae: 0.0742 - val_loss: 0.1040 - val_mae: 0.0963 - learning_rate: 1.0000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0818 - mae: 0.0742 - val_loss: 0.0812 - val_mae: 0.0739 - learning_rate: 1.0000e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0871 - mae: 0.0798 - val_loss: 0.0787 - val_mae: 0.0716 - learning_rate: 1.0000e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0808 - mae: 0.0737 - val_loss: 0.0808 - val_mae: 0.0739 - learning_rate: 1.0000e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0808 - mae: 0.0739 - val_loss: 0.0823 - val_mae: 0.0749 - learning_rate: 1.0000e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0821 - mae: 0.0749 - val_loss: 0.0836 - val_mae: 0.0766 - learning_rate: 1.0000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0778 - mae: 0.0709 - val_loss: 0.0770 - val_mae: 0.0702 - learning_rate: 1.0000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0814 - mae: 0.0746 - val_loss: 0.0848 - val_mae: 0.0776 - learning_rate: 1.0000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0800 - mae: 0.0730 - val_loss: 0.0841 - val_mae: 0.0774 - learning_rate: 1.0000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0823 - mae: 0.0756 - val_loss: 0.0779 - val_mae: 0.0713 - learning_rate: 1.0000e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0809 - mae: 0.0744 - val_loss: 0.0886 - val_mae: 0.0822 - learning_rate: 1.0000e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0807 - mae: 0.0742 - val_loss: 0.0816 - val_mae: 0.0752 - learning_rate: 1.0000e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0778 - mae: 0.0714 - val_loss: 0.0751 - val_mae: 0.0690 - learning_rate: 5.0000e-05\n",
      "Epoch 55/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0778 - mae: 0.0717 - val_loss: 0.0771 - val_mae: 0.0712 - learning_rate: 5.0000e-05\n",
      "Epoch 56/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0748 - mae: 0.0689 - val_loss: 0.0756 - val_mae: 0.0698 - learning_rate: 5.0000e-05\n",
      "Epoch 57/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0774 - mae: 0.0717 - val_loss: 0.0809 - val_mae: 0.0752 - learning_rate: 5.0000e-05\n",
      "Epoch 58/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0758 - mae: 0.0701 - val_loss: 0.0736 - val_mae: 0.0679 - learning_rate: 5.0000e-05\n",
      "Epoch 59/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0749 - mae: 0.0692 - val_loss: 0.0730 - val_mae: 0.0672 - learning_rate: 5.0000e-05\n",
      "Epoch 60/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0755 - mae: 0.0698 - val_loss: 0.0737 - val_mae: 0.0680 - learning_rate: 5.0000e-05\n",
      "Epoch 61/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0771 - mae: 0.0714 - val_loss: 0.0749 - val_mae: 0.0691 - learning_rate: 5.0000e-05\n",
      "Epoch 62/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0762 - mae: 0.0705 - val_loss: 0.0745 - val_mae: 0.0687 - learning_rate: 5.0000e-05\n",
      "Epoch 63/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0765 - mae: 0.0708 - val_loss: 0.0750 - val_mae: 0.0692 - learning_rate: 5.0000e-05\n",
      "Epoch 64/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0775 - mae: 0.0717 - val_loss: 0.0869 - val_mae: 0.0811 - learning_rate: 5.0000e-05\n",
      "Epoch 65/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0771 - mae: 0.0712 - val_loss: 0.0728 - val_mae: 0.0670 - learning_rate: 2.5000e-05\n",
      "Epoch 66/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0738 - mae: 0.0680 - val_loss: 0.0944 - val_mae: 0.0887 - learning_rate: 2.5000e-05\n",
      "Epoch 67/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0716 - mae: 0.0659 - val_loss: 0.0689 - val_mae: 0.0632 - learning_rate: 2.5000e-05\n",
      "Epoch 68/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0751 - mae: 0.0695 - val_loss: 0.0771 - val_mae: 0.0714 - learning_rate: 2.5000e-05\n",
      "Epoch 69/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0759 - mae: 0.0702 - val_loss: 0.0700 - val_mae: 0.0643 - learning_rate: 2.5000e-05\n",
      "Epoch 70/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0715 - mae: 0.0658 - val_loss: 0.0702 - val_mae: 0.0646 - learning_rate: 2.5000e-05\n",
      "Epoch 71/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0726 - mae: 0.0669 - val_loss: 0.0717 - val_mae: 0.0661 - learning_rate: 2.5000e-05\n",
      "Epoch 72/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0729 - mae: 0.0673 - val_loss: 0.0687 - val_mae: 0.0631 - learning_rate: 2.5000e-05\n",
      "Epoch 73/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0716 - mae: 0.0659 - val_loss: 0.0711 - val_mae: 0.0654 - learning_rate: 2.5000e-05\n",
      "Epoch 74/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0755 - mae: 0.0698 - val_loss: 0.0786 - val_mae: 0.0730 - learning_rate: 2.5000e-05\n",
      "Epoch 75/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0750 - mae: 0.0693 - val_loss: 0.0839 - val_mae: 0.0782 - learning_rate: 2.5000e-05\n",
      "Epoch 76/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0719 - mae: 0.0663 - val_loss: 0.0674 - val_mae: 0.0618 - learning_rate: 2.5000e-05\n",
      "Epoch 77/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0747 - mae: 0.0691 - val_loss: 0.0666 - val_mae: 0.0609 - learning_rate: 2.5000e-05\n",
      "Epoch 78/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0721 - mae: 0.0665 - val_loss: 0.0758 - val_mae: 0.0701 - learning_rate: 2.5000e-05\n",
      "Epoch 79/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0725 - mae: 0.0669 - val_loss: 0.0676 - val_mae: 0.0619 - learning_rate: 2.5000e-05\n",
      "Epoch 80/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0728 - mae: 0.0671 - val_loss: 0.0756 - val_mae: 0.0700 - learning_rate: 2.5000e-05\n",
      "Epoch 81/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0733 - mae: 0.0677 - val_loss: 0.0670 - val_mae: 0.0613 - learning_rate: 2.5000e-05\n",
      "Epoch 82/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0736 - mae: 0.0680 - val_loss: 0.0686 - val_mae: 0.0629 - learning_rate: 2.5000e-05\n",
      "Epoch 83/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0725 - mae: 0.0669 - val_loss: 0.0699 - val_mae: 0.0643 - learning_rate: 1.2500e-05\n",
      "Epoch 84/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0695 - mae: 0.0638 - val_loss: 0.0704 - val_mae: 0.0648 - learning_rate: 1.2500e-05\n",
      "Epoch 85/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0718 - mae: 0.0662 - val_loss: 0.0691 - val_mae: 0.0635 - learning_rate: 1.2500e-05\n",
      "Epoch 86/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0695 - mae: 0.0639 - val_loss: 0.0716 - val_mae: 0.0660 - learning_rate: 1.2500e-05\n",
      "Epoch 87/200\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0689 - mae: 0.0633 - val_loss: 0.0703 - val_mae: 0.0647 - learning_rate: 1.2500e-05\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0661 - mae: 0.0605\n",
      "Validation MAE: 0.0609423853456974\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0622 - mae: 0.0565\n",
      "Test MAE: 0.060503896325826645\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model with a chosen optimizer\n",
    "model = OptimizedAOTModel().model(learning_rate=0.0001, optimizer_choice='adam')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "print(\"starting training\")\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    [X_train_img, X_train_num], y_train,\n",
    "    validation_data=([X_val_img, X_val_num], y_val),\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.save('optimized_aot_model.keras')\n",
    "\n",
    "# Evaluate the model\n",
    "val_loss, val_mae = model.evaluate([X_val_img, X_val_num], y_val)\n",
    "print(f'Validation MAE: {val_mae}')\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_mae = model.evaluate([X_test_img, X_test_num], y_test)\n",
    "print(f'Test MAE: {test_mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bit of code initializes and compiles an instance of the OptimizedAOTModel, then trains it using both image and numerical data. It utilizes EarlyStopping and ReduceLROnPlateau callbacks to optimize training and prevent overfitting. After training, the model is saved to a file, and its performance is evaluated on both validation and test datasets. The validation and test mean absolute errors (MAE) are printed to assess the model's accuracy.\n",
    "\n",
    "+ Model Initialization and Compilation\n",
    "\n",
    "    + Model Instantiation and Compilation:\n",
    "        + model = AOTModel(): Creates an instance of the AOTModel class.\n",
    "        + model = model.model(): Calls the model method of the AOTModel instance to create a compiled Keras model.\n",
    "\n",
    "+ Callbacks\n",
    "\n",
    "    + EarlyStopping:\n",
    "        + monitor='val_loss': Monitors the validation loss.\n",
    "        + patience=10: Training stops if the validation loss doesn't improve for 10 consecutive epochs.\n",
    "        + restore_best_weights=True: Restores the model weights from the epoch with the best validation loss.\n",
    "    + ReduceLROnPlateau:\n",
    "        + monitor='val_loss': Monitors the validation loss.\n",
    "        + factor=0.5: Reduces the learning rate by a factor of 0.5 if the validation loss doesn't improve.\n",
    "        + patience=5: Waits for 5 epochs before reducing the learning rate.\n",
    "        + min_lr=1e-6: Ensures that the learning rate doesn't go below 1×10−61×10−6.\n",
    "\n",
    "+ Model Training\n",
    "\n",
    "    + Training the Model:\n",
    "        + model.fit(...): Trains the model using the provided training data.\n",
    "        + [X_train_img, X_train_num]: Input training data consisting of images and numerical features.\n",
    "        + y_train: Training labels.\n",
    "        + validation_data=([X_val_img, X_val_num], y_val): Validation data consisting of images, numerical features, and labels.\n",
    "        + epochs=200: Maximum number of epochs for training.\n",
    "        + batch_size=32: Number of samples per gradient update.\n",
    "        + callbacks=[early_stopping, reduce_lr]: List of callbacks to use during training.\n",
    "        + verbose=1: Verbosity mode for logging the progress of training.\n",
    "\n",
    "+ Model Saving\n",
    "\n",
    "    + Saving the Model:\n",
    "        + model.save('aot_model.keras'): Saves the trained model to a file named aot_model.keras.\n",
    "\n",
    "+ Model Evaluation\n",
    "\n",
    "    + Evaluating the Model on Validation Data:\n",
    "        + val_loss, val_mae = model.evaluate([X_val_img, X_val_num], y_val): Evaluates the model on the validation data and returns the validation loss and mean absolute error (MAE).\n",
    "        + print(f'Validation MAE: {val_mae}'): Prints the validation MAE.\n",
    "\n",
    "    + Evaluating the Model on Test Data:\n",
    "        + test_loss, test_mae = model.evaluate([X_test_img, X_test_num], y_test): Evaluates the model on the test data and returns the test loss and MAE.\n",
    "        + print(f'Test MAE: {test_mae}'): Prints the test MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n",
      "   id  value_550\n",
      "0   3   0.126318\n",
      "1  25   0.100225\n",
      "2  26   0.066412\n",
      "3  27   0.128408\n",
      "4  29   0.073117\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "# model = load_model('aot_model.keras')\n",
    "\n",
    "model = load_model('optimized_aot_model.keras')\n",
    "\n",
    "# Load the new dataset\n",
    "new_df = pd.read_csv(\"datasets/test.csv\")\n",
    "\n",
    "# Preprocess numerical data\n",
    "new_numerical_features = new_df[['elevation', 'ozone', 'NO2', 'azimuth', 'zenith', 'incidence_azimuth', 'incidence_zenith']]\n",
    "new_numerical_features = scaler.transform(new_numerical_features)\n",
    "\n",
    "# Load and preprocess new image data\n",
    "new_image_data = np.array([load_and_preprocess_image(os.path.join('./test/', filename)) for filename in new_df['file_name_l1']])\n",
    "\n",
    "# Predict values for the new data\n",
    "predictions = model.predict([new_image_data, new_numerical_features])\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "results = pd.DataFrame({\n",
    "    'id': new_df['id'],\n",
    "    'value_550': predictions.flatten()\n",
    "})\n",
    "results.to_csv('predictions.csv', index=False)\n",
    "\n",
    "print(results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally here we did the prevision of the test data (here only shown the predictions made by the optimized architecture).\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The neural network architectures presented are specifically designed to predict aerosol optical thickness (AOT) by processing both image and numerical data. Here are the key insights and conclusions based on the provided models and their training procedures:\n",
    "\n",
    "### Effectiveness of Multi-Input Neural Networks\n",
    "The architectures effectively combine convolutional neural networks (CNNs) for image processing with dense layers for numerical data processing. This multi-input approach allows the model to leverage spatial features from satellite or other imaging data alongside additional numerical features, providing a comprehensive understanding of the factors influencing AOT.\n",
    "\n",
    "### Optimization Techniques\n",
    "Several advanced techniques are employed to enhance model performance:\n",
    "- **BatchNormalization**: This helps in stabilizing and accelerating the training process by normalizing the inputs of each layer.\n",
    "- **Dropout**: By introducing dropout layers, the model is better regularized, reducing the risk of overfitting.\n",
    "- **HeUniform Initialization**: This weight initialization technique helps in maintaining a good distribution of weights, which is particularly useful for training deep networks.\n",
    "- **LeakyReLU Activation**: This activation function helps prevent dead neurons, ensuring that all neurons can contribute to learning.\n",
    "\n",
    "### Callbacks for Improved Training\n",
    "The use of callbacks such as `EarlyStopping` and `ReduceLROnPlateau` demonstrates a robust approach to training:\n",
    "- **EarlyStopping**: This helps in stopping the training process once the model stops improving, saving time and computational resources while ensuring the model does not overfit.\n",
    "- **ReduceLROnPlateau**: This reduces the learning rate when the model's performance plateaus, allowing for finer adjustments and potentially better convergence.\n",
    "\n",
    "### Performance Evaluation\n",
    "The training and evaluation process includes careful monitoring of validation and test mean absolute error (MAE), providing clear metrics to assess the model's predictive performance. The reported MAE on validation and test sets offers insights into how well the model generalizes to unseen data, which is crucial for real-world applications.\n",
    "\n",
    "### Application in Aerosol Optical Thickness Prediction\n",
    "The models are well-suited for predicting AOT, a critical parameter in understanding atmospheric conditions and air quality. By accurately predicting AOT, these models can aid in:\n",
    "- **Environmental Monitoring**: Providing timely and accurate information on aerosol concentrations.\n",
    "- **Climate Studies**: Contributing to research on the impact of aerosols on climate change.\n",
    "- **Public Health**: Informing public health initiatives by tracking air quality and its potential effects on respiratory health.\n",
    "\n",
    "### Future Directions\n",
    "While the presented models show promise, future work could explore:\n",
    "- **Enhanced Data Integration**: Incorporating additional data sources such as meteorological data or historical AOT measurements.\n",
    "- **Model Refinement**: Experimenting with different architectures or advanced techniques such as attention mechanisms to further improve accuracy.\n",
    "- **Deployment and Scalability**: Developing scalable solutions for real-time AOT prediction in various geographical regions.\n",
    "\n",
    "In conclusion, the designed neural networks provide a powerful tool for predicting aerosol optical thickness, combining sophisticated data processing techniques with robust training strategies to achieve reliable and accurate predictions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
